#!/usr/bin/env bash
trap 'jobs -p | xargs -r kill' EXIT

current_dir=$(basename "$PWD")
test_dir=$(realpath "$(dirname "$0")")
cd "$test_dir"

export test="$current_dir"

COLORED='\033[0;33m'
NC='\033[0m' # No Color

docker-compose -f ../../docker-compose.yml --env-file ../../.env down
docker-compose -f ../../docker-compose.yml --env-file ../../.env rm -fs

echo -e "${COLORED}Preparing CSV:${NC}"
./prepare_csv/prepare.sh | while IFS= read -r line; do echo -e "\t$line"; done
if [ $? -ne 0 ]; then
  echo -e "\tCouldn't prepare CSV"
  exit 1
fi

../../init --engine=quickwit
../../init --engine=clickhouse
../../init --engine=elasticsearch --type=tuned
../../init --engine=manticoresearch --type=rowwise
../../init --engine=manticoresearch --type=columnar
../../init --engine=mysql --type=tuned


# Can't process dataset this size
# ../../init --engine=typesense

# We excluded Postgres cause it's too slow
# ../../init --engine=postgres --type=tuned

# We excluded Meilisearch engine cause it has limitations and we can upload only 9000000 documents.
# According to docker stats it just reaches the memory limit and then stops indexing 125GiB from 125GiB.

# thread 'indexing-thread:20' panicked at 'the allocator is unable to allocate that
# much memory (41666672 bytes requested)',
# /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/grenad-0.4.4/src/sorter.rs:350:9

# ../../init --engine=meilisearch


# MySQL Percona
if [[ -d mysql_percona ]]; then
  ../../init --engine=mysql_percona
fi
